{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import random\n",
    "import torch.cuda\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import svd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_Net(nn.Module):\n",
    "    def __init__(self, inputs):\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.linear1 = nn.Linear(inputs, 256)\n",
    "        self.linear2 = nn.Linear(256,512)\n",
    "        self.linear3 = nn.Linear(512, 256)\n",
    "        self.linear4 = nn.Linear(256,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.linear1(x))\n",
    "        x = F.tanh(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = F.sigmoid(self.linear4(x))\n",
    "        return x\n",
    "    \n",
    "    def save(self, file_name='model.pth', index=0):\n",
    "        model_folder_path = './Classifier_models/model' \n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        complete_file_name = f\"{index}_{file_name}\"\n",
    "        file_path = os.path.join(model_folder_path, complete_file_name)\n",
    "        \n",
    "        torch.save(self.state_dict(), file_path)\n",
    "    \n",
    "class Classifier_Trainer:\n",
    "    def __init__(self, model, lr):\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        self.device = model.device\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def train_step(self, state, outcome):\n",
    "        state = torch.tensor(np.array(state), dtype=torch.float).to(self.device)\n",
    "\n",
    "        # Make a prediction\n",
    "        pred = self.model(state)[0]\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        ## Calculate MSE loss on target and prediction\n",
    "        loss = self.criterion(pred, outcome[0])\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "## Agent tager filepath som input hvis man vil køre en model, der allerede er trænet.\n",
    "class Agent:\n",
    "    def __init__(self, X, y, file_path=None, training=True, device=None, \n",
    "                 learning_rate=0.01, model_name='testing', pcs=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.pcs = pcs\n",
    "        self.LR = learning_rate\n",
    "        if device is not None: ## Her kan man vælge at køre cpu selvom man har cuda\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        inputs = len(self.X.T) + (len(self.pcs.T) if self.pcs is not None else 0)\n",
    "        self.model = Classifier_Net(inputs=inputs).to(self.device) \n",
    "        self.model_name = model_name\n",
    "\n",
    "        ## Definerer en masse variable baseret på __init__ input\n",
    "        self.is_training = training\n",
    "        self.file_path = file_path\n",
    "\n",
    "        ## Hvis vi har en sti til en model, vil vi loade den i stedet for at træne en ny\n",
    "        if self.file_path is not None:\n",
    "            self.model.load_state_dict(torch.load(self.file_path, map_location=self.device))\n",
    "            self.model.eval()\n",
    "\n",
    "        ## Initialisér trainer\n",
    "        self.trainer = Classifier_Trainer(self.model, lr=self.LR)\n",
    "\n",
    "        ## Træner short memory\n",
    "\n",
    "        ## Bestem en action\n",
    "    def get_action(self, state):\n",
    "\n",
    "        ## Laver state om til tensor og får en prediction fra modellen\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float).to(self.device).clone().detach()\n",
    "        prediction = self.model(state_tensor)\n",
    "        print(prediction.item())\n",
    "\n",
    "        return torch.round(prediction).item()\n",
    "    \n",
    "    def train(self, rounds, evaluating=False, give_preds=False, index=None):\n",
    "        accur = np.zeros(rounds)\n",
    "        lower_acc = np.zeros(rounds)\n",
    "        upper_acc = np.zeros(rounds)\n",
    "        pos_accuracy = np.zeros(rounds)\n",
    "        neg_accuracy = np.zeros(rounds)\n",
    "        pos_outcome_tracker = []\n",
    "        neg_outcome_tracker = []\n",
    "        misclassified_obs = []\n",
    "        decision_list = []\n",
    "        \n",
    "        for j in range(rounds):\n",
    "            res = []\n",
    "            decision_list = []\n",
    "            raw_preds = []\n",
    "            for i in range(len(self.X)):\n",
    "                state = self.X[i]\n",
    "                if self.pcs is not None:\n",
    "                    pc_vals = [pc @ state for pc in self.pcs]\n",
    "                    state = np.concatenate((state, pc_vals))\n",
    "                outcome = self.y[i]\n",
    "                outcome = torch.tensor([float(outcome)]).to(self.device)\n",
    "\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float).to(self.device).clone().detach()\n",
    "                prediction = self.model(state_tensor)\n",
    "                raw_preds.append(prediction.item())\n",
    "                decision = torch.round(prediction).item()\n",
    "\n",
    "                if give_preds:\n",
    "                    decision_list.append(decision)\n",
    "                reward = 1 if decision == outcome else 0\n",
    "\n",
    "                if outcome.item() == 1:\n",
    "                    ## Personen fik diabetes\n",
    "                    if decision == 1: pos_outcome_tracker.append(1)\n",
    "                    else: pos_outcome_tracker.append(0)\n",
    "                else:\n",
    "                    ## Personen fik ikke diabetes\n",
    "                    if decision == 0: neg_outcome_tracker.append(1)\n",
    "                    else: neg_outcome_tracker.append(0)\n",
    "\n",
    "                if self.is_training: \n",
    "                    self.trainer.train_step(state, outcome)\n",
    "                else: \n",
    "                    if reward == 0: misclassified_obs.append(self.X[i])\n",
    "\n",
    "                res.append(reward)\n",
    "\n",
    "            p_hat = np.mean(res)\n",
    "            lower, upper =p_hat - 1.96*np.sqrt((p_hat*(1-p_hat))/len(res)), p_hat + 1.96*np.sqrt((p_hat*(1-p_hat))/len(res))\n",
    "                \n",
    "            accur[j] = np.mean(res)\n",
    "            lower_acc[j], upper_acc[j] = lower, upper\n",
    "        if self.is_training:\n",
    "            self.model.save(index=index if index is not None else 'test')\n",
    "        if give_preds: return decision_list, accur[-1], raw_preds\n",
    "        \n",
    "        if evaluating:\n",
    "            pos_accuracy = np.mean(pos_outcome_tracker)\n",
    "            neg_accuracy = np.mean(neg_outcome_tracker)\n",
    "            return accur[0], lower_acc[0], upper_acc[0], pos_accuracy, neg_accuracy, misclassified_obs\n",
    "\n",
    "        return np.concatenate(([accur], [lower_acc], [upper_acc]), axis=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_nan(DataFrame):\n",
    "    return DataFrame.copy().dropna()\n",
    "\n",
    "def impute_nan(DataFrame, n=None):\n",
    "    global n_nearest_neighbors\n",
    "    imputer = KNNImputer(n_neighbors=(n if n is not None else n_nearest_neighbors))\n",
    "    imputed_array = imputer.fit_transform(DataFrame)\n",
    "    imputed_df = pd.DataFrame(data=imputed_array, columns=DataFrame.columns)\n",
    "    return  imputed_df\n",
    "\n",
    "def remove_outliers(DataFrame):\n",
    "    # Calculate the mean and standard deviation for each column\n",
    "    means = DataFrame.mean()\n",
    "    stds = DataFrame.std()\n",
    "    DataFrame = DataFrame.copy()\n",
    "\n",
    "    global std_threshold\n",
    "\n",
    "    # Create boolean DataFrame indicating whether or not observations exceed threshold\n",
    "    conditions = (DataFrame < (means - std_threshold * stds)) | (DataFrame > (means + std_threshold * stds))\n",
    "\n",
    "    # Any row that should be removed will have at least one True in the conditions DataFrame\n",
    "    rows_to_remove = conditions.any(axis=1)\n",
    "\n",
    "    # Remove the rows that meet the condition\n",
    "    DataFrame = DataFrame[~rows_to_remove]\n",
    "    return DataFrame\n",
    "\n",
    "def remove_misclassified_observations(DataFrame):\n",
    "    global features\n",
    "\n",
    "    ## Prepare data for clustering\n",
    "    k_df = remove_outliers(remove_nan(DataFrame))\n",
    "\n",
    "    ## Instantialize the KMeans class and fit it on the data\n",
    "    kmeans = KMeans(n_clusters=2, n_init=50, max_iter=10_000)\n",
    "    kmeans.fit(k_df[features])\n",
    "\n",
    "    ## Give the dataframe a column with the labels from the clusterer\n",
    "    k_df = k_df.assign(cluster=kmeans.labels_)\n",
    "\n",
    "    ## Map the outcome to a cluster\n",
    "    mapping = k_df.groupby('cluster')['Outcome'].agg(lambda x: x.value_counts().index[0]).to_dict()\n",
    "    k_df['cluster_mapped_to_class'] = k_df['cluster'].map(mapping)\n",
    "\n",
    "    ## Create a boolean column, indicating whether or not the observation was misclassified\n",
    "    k_df['misclassified'] = k_df['Outcome'] != k_df['cluster_mapped_to_class']\n",
    "    misclassified_df = k_df[k_df['misclassified']]\n",
    "\n",
    "    print('Accuracy of clusterer', round(100-len(misclassified_df)/len(k_df)*100,2),'%')\n",
    "\n",
    "    ## Filter the dataframe for misclassified observations\n",
    "    misclassified_indices = misclassified_df.index\n",
    "    return DataFrame.copy().drop(misclassified_indices)\n",
    "\n",
    "def get_cluster_df(DataFrame):\n",
    "    global features\n",
    "\n",
    "    df_copy = DataFrame.copy()\n",
    "    cluster_list = np.full((len(df_copy),1), fill_value=-1)\n",
    "\n",
    "    ## Prepare data for clustering\n",
    "    k_df = remove_outliers(remove_nan(DataFrame))\n",
    "\n",
    "    ## Instantialize the KMeans class and fit it on the data\n",
    "    kmeans = KMeans(n_clusters=2, n_init=50, max_iter=10_000)\n",
    "    kmeans.fit(k_df[features])\n",
    "\n",
    "    ## Give the dataframe a column with the labels from the clusterer\n",
    "    k_df = k_df.assign(cluster=kmeans.labels_)\n",
    "\n",
    "    ## Map the outcome to a cluster\n",
    "    mapping = k_df.groupby('cluster')['Outcome'].agg(lambda x: x.value_counts().index[0]).to_dict()\n",
    "    k_df['cluster_mapped_to_class'] = k_df['cluster'].map(mapping)\n",
    "    indices = k_df.index\n",
    "    for idx in indices:\n",
    "        cluster_list[idx] = k_df.loc[idx][len(k_df.columns)-1]\n",
    "    df_copy = df_copy.assign(Cluster=cluster_list)\n",
    "    return df_copy\n",
    "\n",
    "def get_PCs(x):\n",
    "    # Create an instance of the PCA class\n",
    "    pca = PCA()\n",
    "\n",
    "    # Apply PCA to the attributes\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "    # Perform singular value decomposition\n",
    "    Y = x - np.ones((x.shape[0], 1)) * x.mean(0)\n",
    "    U, S, Vh = svd(Y, full_matrices=False)\n",
    "    V = Vh.T\n",
    "    return Vh\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def k_split(k, DataFrame):\n",
    "    global target_variable\n",
    "    ## Shuffle the DataFrame\n",
    "    shuffled = DataFrame.sample(frac=1)\n",
    "\n",
    "    ## Split into k groups\n",
    "    groups = np.array_split(shuffled, k)\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    pc_array = []\n",
    "\n",
    "    for i in range(k):\n",
    "        groups_copy = groups.copy()\n",
    "        test_data = groups_copy.pop(i)\n",
    "\n",
    "        training_frames = pd.concat(groups_copy)\n",
    "\n",
    "        training_standard = training_frames.loc[:, features].values\n",
    "\n",
    "        scaler = StandardScaler().fit(training_standard)\n",
    "\n",
    "        ## Transofmr training data\n",
    "        training_standard = scaler.transform(training_standard)\n",
    "\n",
    "        X_train.append(training_standard)\n",
    "        y_train.append(np.array(training_frames[target_variable]))\n",
    "\n",
    "        test_standard = test_data.loc[:, features].values\n",
    "\n",
    "        ## Standardize the test data according to the mean and std of training data\n",
    "        test_standard = scaler.transform(test_standard)\n",
    "\n",
    "        X_test.append(test_standard)\n",
    "        y_test.append(np.array(test_data[target_variable]))\n",
    "\n",
    "       ## Perform PCA\n",
    "        Vh = get_PCs(training_standard)\n",
    "        pc_array.append(Vh)\n",
    "    return X_train, y_train, X_test, y_test, np.array(pc_array)\n",
    "\n",
    "def cluster(X_train, y_train, X_test):\n",
    "    'Returns (training data predictions, test data predictions)'\n",
    "    global target_variable\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(X_train)\n",
    "    train_cluster_labels = kmeans.labels_\n",
    "    df_train = pd.DataFrame({'Cluster': train_cluster_labels, 'Outcome_lab': y_train})\n",
    "    cluster_positive_label = df_train.groupby('Cluster')['Outcome_lab'].mean().idxmax()\n",
    "    cluster_col_train = np.array(df_train['Cluster'])\n",
    "    train_cluster = cluster_col_train if cluster_positive_label else (cluster_col_train + 1) % 2\n",
    "\n",
    "    test_cluster_labels = kmeans.predict(X_test)\n",
    "    test_cluster_preds = (test_cluster_labels == cluster_positive_label).astype(int)\n",
    "\n",
    "    # Return array of train clusters and array of prediction of clusters on test data\n",
    "    return train_cluster, test_cluster_preds\n",
    "\n",
    "def naive(X_train, y_train, X_test):\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    x_pred = model.predict(X_train)\n",
    "    return x_pred, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster accuracy 0.7362924281984334\n",
      "Naive accuracy 0.7650130548302873\n"
     ]
    }
   ],
   "source": [
    "# Read csv file to pd DataFrame\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "\n",
    "## Data preparation\n",
    "for col in df.columns:\n",
    "    if col in ['Pregnancies', 'Outcome']:\n",
    "        continue\n",
    "    df[col] = df[col].replace(0, np.nan)\n",
    "\n",
    "target_variable = 'Outcome'\n",
    "\n",
    "drop_features = []#['Insulin', 'SkinThickness']\n",
    "\n",
    "drop_columns = np.concatenate(([target_variable], drop_features))\n",
    "\n",
    "df = df.drop(columns=drop_features)\n",
    "\n",
    "features = [col for col in df.columns if col not in [kol for kol in drop_columns]]\n",
    "\n",
    "std_threshold = 4\n",
    "\n",
    "df = remove_outliers(remove_nan(df))\n",
    "\n",
    "k = 5\n",
    "\n",
    "X_train, y_train, X_test, y_test, pc_array = k_split(k,df)\n",
    "\n",
    "clusters = []\n",
    "naive_preds = []\n",
    "for xtrain, ytrain, xtest in zip(X_train, y_train, X_test):\n",
    "    clusters.append((cluster(xtrain, ytrain, xtest)))\n",
    "    naive_preds.append(naive(xtrain,ytrain,xtest))\n",
    "\n",
    "acc_c, compar_c = [], []\n",
    "acc_n, compar_n = [], []\n",
    "for i in range(len(y_test)):\n",
    "        compar_c.append(np.column_stack((clusters[i][1], y_test[i])))\n",
    "        compar_n.append(np.column_stack((naive_preds[i][1], y_test[i])))\n",
    "        if i != 0:\n",
    "            compar_arr = np.concatenate((compar_arr, compar_c[i]))\n",
    "            compar_n_arr = np.concatenate((compar_n_arr, compar_n[i]))\n",
    "        else: \n",
    "            compar_arr = compar_c[0]\n",
    "            compar_n_arr = compar_n[0]\n",
    "\n",
    "for comp, comp_n in zip(compar_arr,compar_n_arr):\n",
    "    acc_c.append(1 if comp[0] == comp[1] else 0)\n",
    "    acc_n.append(1 if comp_n[0] == comp_n[1] else 0)\n",
    "print('Cluster accuracy', np.mean(acc_c))\n",
    "print('Naive accuracy', np.mean(acc_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0: 81.818%, (Train Accuracy 0: 78.431%)\n",
      "Accuracy 1: 70.13%, (Train Accuracy 1: 81.699%)\n",
      "Accuracy 2: 83.117%, (Train Accuracy 2: 78.105%)\n",
      "Accuracy 3: 77.632%, (Train Accuracy 3: 82.085%)\n",
      "Accuracy 4: 76.316%, (Train Accuracy 4: 79.479%)\n",
      "Total Network accuracy: 77.802%\n"
     ]
    }
   ],
   "source": [
    "LR = 0.00001\n",
    "rtt = 10\n",
    "nn_acc = []\n",
    "nn_train_pred_list = []\n",
    "nn_test_pred_list = []\n",
    "nn_train_round_pred_list = []\n",
    "\n",
    "full_frames_train = []\n",
    "full_frames_test = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    agent = Agent(X_train[i], y_train[i], learning_rate=LR)\n",
    "    train_preds, train_acc, raw_train_preds = agent.train(rtt, give_preds=True, index=i)\n",
    "    nn_train_pred_list.append(raw_train_preds)\n",
    "    nn_train_round_pred_list.append(train_preds)\n",
    "\n",
    "    agent = Agent(X_test[i], y_test[i], file_path=f'Classifier_models/model/{i}_model.pth',training=False)\n",
    "    nn_preds, test_acc, raw_preds = agent.train(1, evaluating=True, give_preds=True)\n",
    "    nn_acc.append(test_acc)\n",
    "    nn_test_pred_list.append(raw_preds)\n",
    "\n",
    "    print(f'Accuracy {i}: {round(test_acc*100,3)}%, (Train Accuracy {i}: {round(train_acc*100,3)}%)')\n",
    "    full_frames_train.append(pd.DataFrame(np.column_stack((X_train[i], clusters[i][0], naive_preds[i][0], \n",
    "                                                   raw_train_preds, y_train[i])), \n",
    "                                                    columns=np.concatenate((\n",
    "                                                        features,['Cluster'], ['Naive'], \n",
    "                                                        ['Network'],[target_variable])).flatten()))\n",
    "    full_frames_test.append(pd.DataFrame(np.column_stack((X_test[i], clusters[i][1], naive_preds[i][1], \n",
    "                                                   raw_preds, y_test[i])), \n",
    "                                                    columns=np.concatenate((\n",
    "                                                        features,['Cluster'], ['Naive'], \n",
    "                                                        ['Network'],[target_variable])).flatten()))\n",
    "    \n",
    "print(f'Total Network accuracy: {np.round(np.mean(nn_acc)*100,3)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster accuracy: 73.629% \n",
      "Naive accuracy: 76.501% \n",
      "Network accuracy: 77.802% \n",
      "Average of above: 76.0%\n",
      "Ensemble: 76.501%\n"
     ]
    }
   ],
   "source": [
    "test_ensemble_predictions = []\n",
    "test_raw_ensemble_predictions = []\n",
    "train_ensemble_predictions = []\n",
    "train_raw_ensemble_predictions = []\n",
    "for i in range(k):\n",
    "    cluster_pred, naive_pred, network_pred, outcome_actual = (full_frames_test[i]['Cluster'].values, \n",
    "                                                              full_frames_test[i]['Naive'].values,\n",
    "                                                              full_frames_test[i]['Network'].values,\n",
    "                                                              full_frames_test[i]['Outcome'].values)\n",
    "    \n",
    "    cluster_pred_t, naive_pred_t, network_pred_t, outcome_actual_t = (full_frames_train[i]['Cluster'].values, \n",
    "                                                              full_frames_train[i]['Naive'].values,\n",
    "                                                              full_frames_train[i]['Network'].values,\n",
    "                                                              full_frames_train[i]['Outcome'].values)\n",
    "\n",
    "    all_preds = []\n",
    "    raw_ensemble_preds = []\n",
    "    for j in range(len(full_frames_test[i])):\n",
    "        curr_obs = [cluster_pred[j], naive_pred[j], network_pred[j]]\n",
    "        mean_obs = np.mean(curr_obs)\n",
    "        mean_obs_round = np.round(mean_obs)\n",
    "        all_preds.append(1 if mean_obs_round == outcome_actual[j] else 0)\n",
    "        raw_ensemble_preds.append(mean_obs)\n",
    "    test_ensemble_predictions.append(all_preds)\n",
    "    test_raw_ensemble_predictions.append(raw_ensemble_preds)\n",
    "\n",
    "    all_preds_t = []\n",
    "    raw_ensemble_preds_t = []\n",
    "    for j in range(len(full_frames_train[i])):\n",
    "        curr_obs_t = [cluster_pred_t[j], naive_pred_t[j], network_pred_t[j]]\n",
    "        mean_obs_t = np.mean(curr_obs_t)\n",
    "        mean_obs_round_t = np.round(mean_obs_t)\n",
    "        all_preds_t.append(1 if mean_obs_round_t == outcome_actual_t[j] else 0)\n",
    "        raw_ensemble_preds_t.append(mean_obs_t)\n",
    "    train_ensemble_predictions.append(all_preds_t)\n",
    "    train_raw_ensemble_predictions.append(raw_ensemble_preds_t)\n",
    "\n",
    "ensemble_temp = np.concatenate([seq for seq in test_ensemble_predictions])\n",
    "ensemble_acc = np.mean(ensemble_temp)\n",
    "print(f'Cluster accuracy: {np.round(np.mean(acc_c)*100,3)}%',\n",
    "      f'\\nNaive accuracy: {np.round(np.mean(acc_n)*100,3)}%',\n",
    "      f'\\nNetwork accuracy: {np.round(np.mean(nn_acc)*100,3)}%',\n",
    "      f'\\nAverage of above: {np.round(np.mean([np.round(np.mean(nn_acc)*100,3), np.round(np.mean(acc_n)*100,3), np.round(np.mean(acc_c)*100,3)]))}%'\n",
    "      f'\\nEnsemble: {np.round(ensemble_acc*100,3)}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pc, X_test_pc = [], []\n",
    "pc_train_df, pc_test_df = [], []\n",
    "for train_block, test_block, pcsarr in zip(X_train, X_test, pc_array):\n",
    "    xtr, xte = [], []\n",
    "    for obs in train_block:\n",
    "        first_three = [obs @ pc for pc in pcsarr]\n",
    "        xtr.append(first_three)\n",
    "    for obs in test_block:\n",
    "        first_three = [obs @ pc for pc in pcsarr]\n",
    "        xte.append(first_three)\n",
    "\n",
    "    colnames = [f'PC{i+1}' for i in range(len(pc_array.T))]\n",
    "    pc_train_df.append(pd.DataFrame(xtr, columns=colnames))\n",
    "    pc_test_df.append(pd.DataFrame(xte, columns=colnames))\n",
    "    X_train_pc.append(xtr)\n",
    "    X_test_pc.append(xte)\n",
    "\n",
    "\n",
    "\n",
    "decision_df_train = full_frames_train.copy()\n",
    "for i, dataframe in enumerate(decision_df_train):\n",
    "    dataframe = dataframe.assign(Ensemble=train_raw_ensemble_predictions[i])\n",
    "    dataframe = pd.concat([dataframe,pc_train_df[i]], axis=1)\n",
    "    for feat in features:\n",
    "        dataframe = dataframe.drop(columns=feat)\n",
    "    dataframe = dataframe.drop(columns=target_variable)\n",
    "    decision_df_train[i] = dataframe\n",
    "\n",
    "decision_df_test = full_frames_test.copy()\n",
    "for i, dataframe in enumerate(decision_df_test):\n",
    "    dataframe = dataframe.assign(Ensemble=test_raw_ensemble_predictions[i])\n",
    "    dataframe = pd.concat([dataframe,pc_test_df[i]], axis=1)\n",
    "    for feat in features:\n",
    "        dataframe = dataframe.drop(columns=feat)\n",
    "    dataframe = dataframe.drop(columns=target_variable)\n",
    "    decision_df_test[i] = dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 77.124% Test: 81.818%\n",
      "Train: 79.085% Test: 71.429%\n",
      "Train: 78.105% Test: 79.221%\n",
      "Train: 80.456% Test: 76.316%\n",
      "Train: 78.827% Test: 75.0%\n",
      "Accuracy: 76.757%\n"
     ]
    }
   ],
   "source": [
    "LR_2 = 0.00001\n",
    "rtt_2 = 10\n",
    "\n",
    "nn_train_pred_list_2 = []\n",
    "nn_train_round_pred_list_2 = []\n",
    "nn_acc_2 = []\n",
    "nn_test_pred_list_2 = []\n",
    "\n",
    "naive_preds_2 = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    X_train_2 = decision_df_train[i].values\n",
    "    agent = Agent(X_train_2, y_train[i], learning_rate=LR_2)\n",
    "    train_preds_2, train_acc_2, raw_train_preds_2 = agent.train(rtt_2, give_preds=True, index=f'{i}_2nd')\n",
    "    nn_train_pred_list_2.append(raw_train_preds_2)\n",
    "    nn_train_round_pred_list_2.append(train_preds_2)\n",
    "\n",
    "    X_test_2 = decision_df_test[i].values\n",
    "    agent = Agent(X_test_2, y_test[i], file_path=f'Classifier_models/model/{i}_2nd_model.pth',training=False)\n",
    "    nn_preds_2, test_acc_2, raw_preds_2 = agent.train(1, evaluating=True, give_preds=True)\n",
    "    nn_acc_2.append(test_acc_2)\n",
    "    nn_test_pred_list_2.append(raw_preds_2)\n",
    "    print(f'Train: {round(train_acc_2*100,3)}%', f'Test: {round(test_acc_2*100,3)}%')\n",
    "\n",
    "    naive_preds_2.append(naive(X_train_2,y_train[i],X_test_2))\n",
    "\n",
    "print(f'Accuracy: {round(np.mean(nn_acc_2)*100,3)}%')\n",
    "\n",
    "acc_n_2, compar_n_2 = [], []\n",
    "for i in range(len(y_test)):\n",
    "        compar_n_2.append(np.column_stack((naive_preds_2[i][1], y_test[i])))\n",
    "        if i != 0:\n",
    "            compar_n_arr_2 = np.concatenate((compar_n_arr_2, compar_n_2[i]))\n",
    "        else: \n",
    "            compar_n_arr_2 = compar_n_2[0]\n",
    "\n",
    "for comp_n_2 in compar_n_arr_2:\n",
    "    acc_n_2.append(1 if comp_n_2[0] == comp_n_2[1] else 0)\n",
    "\n",
    "scores = np.array([[np.round(np.mean(acc_c)*100,3)],\n",
    "                           [np.round(np.mean(acc_n)*100,3)],\n",
    "                           [np.round(np.mean(nn_acc)*100,3)],\n",
    "                           [np.round(ensemble_acc*100,3)],\n",
    "                           [np.round(np.mean(nn_acc_2)*100,3)],\n",
    "                           [np.round(np.mean(acc_n_2)*100,3)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Naive</th>\n",
       "      <th>Network1</th>\n",
       "      <th>Ensemble</th>\n",
       "      <th>Network2</th>\n",
       "      <th>Naive2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.629</td>\n",
       "      <td>76.501</td>\n",
       "      <td>77.802</td>\n",
       "      <td>76.501</td>\n",
       "      <td>76.757</td>\n",
       "      <td>77.023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cluster   Naive  Network1  Ensemble  Network2  Naive2\n",
       "0   73.629  76.501    77.802    76.501    76.757  77.023"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performers = pd.DataFrame(scores.T, columns=['Cluster', 'Naive', 'Network1', 'Ensemble', 'Network2', 'Naive2'])\n",
    "performers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
