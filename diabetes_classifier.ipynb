{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import torch.cuda\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_Net(nn.Module):\n",
    "    def __init__(self, inputs):\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.linear1 = nn.Linear(inputs, 256)\n",
    "        self.linear2 = nn.Linear(256,512)\n",
    "        self.linear3 = nn.Linear(512, 256)\n",
    "        self.linear4 = nn.Linear(256,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.linear1(x))\n",
    "        x = F.tanh(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = F.sigmoid(self.linear4(x))\n",
    "        return x\n",
    "    \n",
    "    def save(self, file_name='model.pth', index=0):\n",
    "        model_folder_path = './Classifier_models/model' \n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        complete_file_name = f\"{index}_{file_name}\"\n",
    "        file_path = os.path.join(model_folder_path, complete_file_name)\n",
    "        \n",
    "        torch.save(self.state_dict(), file_path)\n",
    "    \n",
    "class Classifier_Trainer:\n",
    "    def __init__(self, model, lr):\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        self.device = model.device\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def train_step(self, state, outcome):\n",
    "        state = torch.tensor(np.array(state), dtype=torch.float).to(self.device)\n",
    "\n",
    "        # Make a prediction\n",
    "        pred = self.model(state)[0]\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        ## Calculate MSE loss on target and prediction\n",
    "        loss = self.criterion(pred, outcome[0])\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "## Agent tager filepath som input hvis man vil køre en model, der allerede er trænet.\n",
    "class Agent:\n",
    "    def __init__(self, inputs, file_path=None, training=True, device=None, \n",
    "                 learning_rate=0.01, model_name='testing', data=None):\n",
    "        self.data = data\n",
    "        self.MAX_MEMORY = 5_000 ## Længde af buffer\n",
    "        self.memory = deque(maxlen=self.MAX_MEMORY)  ## popleft() buffer\n",
    "        self.BATCH_SIZE = 32 ## Sample størrelse\n",
    "        self.LR = learning_rate ## Learning rate (TIDLIGERE 0.01 for onestep)\n",
    "        if device is not None: ## Her kan man vælge at køre cpu selvom man har cuda\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        inputs = inputs + len(self.data.T) -1\n",
    "        self.model = Classifier_Net(inputs=inputs).to(self.device) \n",
    "        self.model_name = model_name\n",
    "\n",
    "        ## Definerer en masse variable baseret på __init__ input\n",
    "        self.is_training = training\n",
    "        self.file_path = file_path\n",
    "\n",
    "        ## Hvis vi har en sti til en model, vil vi loade den i stedet for at træne en ny\n",
    "        if self.file_path is not None:\n",
    "            self.model.load_state_dict(torch.load(self.file_path, map_location=self.device))\n",
    "            self.model.eval()\n",
    "\n",
    "        ## Initialisér trainer\n",
    "        self.trainer = Classifier_Trainer(self.model, lr=self.LR)\n",
    "\n",
    "        ## Træner short memory\n",
    "    def train_short_memory(self, state, outcome):\n",
    "        self.trainer.train_step(state, outcome)\n",
    "\n",
    "        ## Bestem en action\n",
    "    def get_action(self, state):\n",
    "\n",
    "        ## Laver state om til tensor og får en prediction fra modellen\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float).to(self.device).clone().detach()\n",
    "        prediction = self.model(state_tensor)\n",
    "\n",
    "        return torch.round(prediction).item()\n",
    "    \n",
    "    def train(self, rounds, pcs, loading=False, evaluating=False, give_preds=False, index=None):\n",
    "        accur = np.zeros(rounds)\n",
    "        lower_acc = np.zeros(rounds)\n",
    "        upper_acc = np.zeros(rounds)\n",
    "        pos_accuracy = np.zeros(rounds)\n",
    "        neg_accuracy = np.zeros(rounds)\n",
    "        pos_outcome_tracker = []\n",
    "        neg_outcome_tracker = []\n",
    "        misclassified_obs = []\n",
    "        decision_list = []\n",
    "        for j in range(rounds):\n",
    "            res = []\n",
    "            decision_list = []\n",
    "            for i in range(len(self.data)):\n",
    "                state = self.data[i,:len(self.data.T)-1]\n",
    "                pc_vals = [pc @ state for pc in pcs]\n",
    "                state = np.concatenate((state, pc_vals))\n",
    "                outcome = self.data[i,-1]\n",
    "                outcome = torch.tensor([float(outcome)]).to(self.device)\n",
    "                decision = self.get_action(state)\n",
    "                if give_preds:\n",
    "                    decision_list.append(decision)\n",
    "                reward = 1 if decision == outcome else 0\n",
    "\n",
    "                if outcome.item() == 1:\n",
    "                    ## Personen fik diabetes\n",
    "                    if decision == 1: pos_outcome_tracker.append(1)\n",
    "                    else: pos_outcome_tracker.append(0)\n",
    "                else:\n",
    "                    ## Personen fik ikke diabetes\n",
    "                    if decision == 0: neg_outcome_tracker.append(1)\n",
    "                    else: neg_outcome_tracker.append(0)\n",
    "\n",
    "                if self.is_training: \n",
    "                    self.train_short_memory(state, outcome=outcome)\n",
    "                else: \n",
    "                    if reward == 0: misclassified_obs.append(self.data[i])\n",
    "\n",
    "                res.append(reward)\n",
    "\n",
    "            p_hat = np.mean(res)\n",
    "            lower, upper =p_hat - 1.96*np.sqrt((p_hat*(1-p_hat))/len(res)), p_hat + 1.96*np.sqrt((p_hat*(1-p_hat))/len(res))\n",
    "\n",
    "            if j % (rounds/20) == 0 and loading:\n",
    "                print(f'Loading: {int((j)/rounds*100)}%'*self.is_training,'Mean', round(np.mean(res),2), \n",
    "                      'CI:', round(lower,2), round(upper,2))\n",
    "                \n",
    "            accur[j] = np.mean(res)\n",
    "            lower_acc[j], upper_acc[j] = lower, upper\n",
    "        if self.is_training:\n",
    "            self.model.save(index=index if index is not None else 'test')\n",
    "        if give_preds: return decision_list, accur[0]\n",
    "        \n",
    "        if evaluating:\n",
    "            pos_accuracy = np.mean(pos_outcome_tracker)\n",
    "            neg_accuracy = np.mean(neg_outcome_tracker)\n",
    "            return accur[0], lower_acc[0], upper_acc[0], pos_accuracy, neg_accuracy, misclassified_obs\n",
    "\n",
    "        return np.concatenate(([accur], [lower_acc], [upper_acc]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file to pd DataFrame\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Replace 0's with NaN in columns where 0's are not possible\n",
    "for col in df.columns:\n",
    "    if col in ['Pregnancies', 'Outcome']:\n",
    "        continue\n",
    "    df[col] = df[col].replace(0, np.nan)\n",
    "\n",
    "# Remove NaN rows in data\n",
    "def remove_nan(DataFrame):\n",
    "    return DataFrame.copy().dropna()\n",
    "\n",
    "def impute_nan(DataFrame, n=None):\n",
    "    global n_nearest_neighbors\n",
    "    imputer = KNNImputer(n_neighbors=(n if n is not None else n_nearest_neighbors))\n",
    "    imputed_array = imputer.fit_transform(DataFrame)\n",
    "    imputed_df = pd.DataFrame(data=imputed_array, columns=DataFrame.columns)\n",
    "    return  imputed_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(DataFrame):\n",
    "    # Calculate the mean and standard deviation for each column\n",
    "    means = DataFrame.mean()\n",
    "    stds = DataFrame.std()\n",
    "    DataFrame = DataFrame.copy()\n",
    "\n",
    "    global std_threshold\n",
    "\n",
    "    # Create boolean DataFrame indicating whether or not observations exceed threshold\n",
    "    conditions = (DataFrame < (means - std_threshold * stds)) | (DataFrame > (means + std_threshold * stds))\n",
    "\n",
    "    # Any row that should be removed will have at least one True in the conditions DataFrame\n",
    "    rows_to_remove = conditions.any(axis=1)\n",
    "\n",
    "    # Remove the rows that meet the condition\n",
    "    DataFrame = DataFrame[~rows_to_remove]\n",
    "    return DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into k groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_split(DataFrame, k, RemoveOutliers=False, ImputeMissing=False, log_transform=None, remove_misclassified = False,\n",
    "            add_clustering=False):\n",
    "    'If not ImputeMissing, NaNs will be removed. NaNs will always be removed in test data.'\n",
    "\n",
    "    ## Get the global features and drop_columns variables\n",
    "    global features\n",
    "    global drop_columns\n",
    "    global split_clustering\n",
    "    split_clustering = False\n",
    "    if add_clustering:\n",
    "        split_clustering = True\n",
    "    \n",
    "    len_pre = len(DataFrame)\n",
    "\n",
    "    DataFrame = DataFrame.copy()\n",
    "\n",
    "    ## Removing observations misclassified by the k-means clustering algorithm\n",
    "    if remove_misclassified:\n",
    "        DataFrame = remove_misclassified_observations(DataFrame=DataFrame)\n",
    "\n",
    "    ## Log transformation\n",
    "    if log_transform is not None:\n",
    "        for col in log_transform:\n",
    "            ## Make sure that we are not log-transforming non-existent columns\n",
    "            if col in features:\n",
    "                DataFrame[col] = DataFrame[col].apply(lambda x: np.log1p(x))\n",
    "\n",
    "    ## Remove unwanted columns\n",
    "    unwanted = []\n",
    "    for feature in drop_columns:\n",
    "        if not feature == 'Outcome':\n",
    "            unwanted.append(feature)\n",
    "    DataFrame = DataFrame.drop(columns=unwanted)\n",
    "\n",
    "    ## Shuffle the DataFrame\n",
    "    shuffled = DataFrame.sample(frac=1)\n",
    "\n",
    "    ## Split into k groups\n",
    "    groups = np.array_split(shuffled, k)\n",
    "\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    pc_array = []\n",
    "\n",
    "\n",
    "    for i in range(k):\n",
    "        groups_copy = groups.copy()\n",
    "        test_data = groups_copy.pop(i)\n",
    "\n",
    "        training_frames = pd.concat(groups_copy)\n",
    "\n",
    "        if ImputeMissing:\n",
    "            training_frames = impute_nan(DataFrame=training_frames)\n",
    "        else:\n",
    "            training_frames = remove_nan(DataFrame=training_frames)\n",
    "        if RemoveOutliers:\n",
    "            training_frames = remove_outliers(DataFrame=training_frames)\n",
    "\n",
    "        if add_clustering:\n",
    "            kmeans = KMeans(n_clusters=2, random_state=0).fit(training_frames)\n",
    "            train_cluster_labels = kmeans.labels_\n",
    "\n",
    "            df_train = pd.DataFrame({'Cluster': train_cluster_labels, 'Outcome_lab': training_frames['Outcome']})\n",
    "            cluster_positive_label = df_train.groupby('Cluster')['Outcome_lab'].mean().idxmax()\n",
    "            cluster_col_train = np.array(df_train['Cluster'])\n",
    "            cluster_col_train = cluster_col_train if cluster_positive_label else (cluster_col_train + 1) % 2\n",
    "\n",
    "\n",
    "        training_standard = training_frames.loc[:, features].values\n",
    "\n",
    "        ## Create instance of StandardScaler and fit it on the training data\n",
    "        ## by fitting on training data, we ensure avoidance of information leakage from test data\n",
    "        scaler = StandardScaler().fit(training_standard)\n",
    "\n",
    "        ## Transofmr training data\n",
    "        training_standard = scaler.transform(training_standard)\n",
    "        \n",
    "        ## Add Outcome column to training data\n",
    "        if add_clustering:\n",
    "            training_standard = np.column_stack((training_standard, cluster_col_train))\n",
    "        training_standard = np.column_stack((training_standard, training_frames['Outcome']))\n",
    "        X_data.append(training_standard)\n",
    "\n",
    "        ## We do not want to impute the NaNs on the data, we are using to evaluate the model.\n",
    "        ## In this case, NaNs will be removed. Outliers will stay - may want to change that.\n",
    "        test_data = remove_nan(DataFrame=test_data)\n",
    "        if RemoveOutliers:\n",
    "            test_data = remove_outliers(DataFrame=test_data)\n",
    "\n",
    "        if add_clustering:\n",
    "            test_cluster_labels = kmeans.predict(test_data)\n",
    "            test_class_predictions = (test_cluster_labels == cluster_positive_label).astype(int)\n",
    "\n",
    "        test_standard = test_data.loc[:, features].values\n",
    "\n",
    "        ## Standardize the test data according to the mean and std of training data\n",
    "        test_standard = scaler.transform(test_standard)\n",
    "\n",
    "        ## Add Outcome to test data\n",
    "        if add_clustering:\n",
    "            test_standard = np.column_stack((test_standard, test_class_predictions))\n",
    "        test_standard = np.column_stack((test_standard, test_data['Outcome']))\n",
    "        Y_data.append(test_standard)\n",
    "\n",
    "        ## Perform PCA\n",
    "        %run pca.ipynb \n",
    "        data_drop_outcome = np.array([row[:-1] for row in X_data[i]])\n",
    "        Vh = get_PCs(data_drop_outcome)\n",
    "        pc_array.append(Vh)\n",
    "    len_post = len(X_data[0]) + len(Y_data[0])\n",
    "    print('Length of dataset:', len_pre,\n",
    "            '\\nLength after processing:', len_post,\n",
    "            '\\nObservations removed:', len_pre-len_post)\n",
    "    return X_data, Y_data, np.array(pc_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the cross-validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(training_data, test_data, pcs, n_pcs, learning_rate, rtt, plot=False, loading=False):\n",
    "    print(f'Working on:', 'CUDA' if torch.cuda.is_available() else 'CPU')\n",
    "    validation_stats = []\n",
    "    mco_frames = []\n",
    "    for i in range(len(training_data)):\n",
    "        X= training_data[i]\n",
    "        Y = test_data[i]\n",
    "\n",
    "        components = [pcs[i][j] for j in range(n_pcs)]\n",
    "\n",
    "\n",
    "        agent = Agent(learning_rate=learning_rate, data=X, inputs=n_pcs)\n",
    "        training_accuracy = agent.train(rtt, loading=loading, pcs=components, index=i)\n",
    "        if plot:\n",
    "            sns.lineplot(pd.DataFrame(training_accuracy.T, columns=['Accuracy', 'CI Lower Bound', 'CI Upper Bound']))\n",
    "            plt.xlabel('Rounds')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Training Accuracy')\n",
    "\n",
    "        if loading: print('\\nEval')\n",
    "        agent = Agent(training=False, file_path=f'Classifier_models/model/{i}_model.pth', data=Y, inputs=n_pcs)\n",
    "        final_stats = agent.train(1, loading=loading, pcs=components, evaluating=True)\n",
    "        final_accuracy, final_lower_bound, final_upper_bound, positive_acc, negative_acc, mco = final_stats\n",
    "        print(i+1,round(final_accuracy,2))\n",
    "        print('False Negative %', np.round(1-positive_acc,2), 'False Positive %', np.round(1-negative_acc,2),'\\n')\n",
    "        print('Misclassified observations:', len(mco), '/', len(Y))\n",
    "\n",
    "        global features\n",
    "        global split_clustering\n",
    "        if split_clustering:\n",
    "            mco_frames.append(pd.DataFrame(mco, columns=np.concatenate((features, ['Cluster'], ['Outcome']), axis=0)))\n",
    "        else:\n",
    "            mco_frames.append(pd.DataFrame(mco, columns=np.concatenate((features, ['Outcome']), axis=0)))\n",
    "\n",
    "        if plot:\n",
    "            plt.errorbar(x=rtt, y= final_accuracy, yerr=(final_upper_bound-final_lower_bound)/2, elinewidth=1, \n",
    "            capsize=10)\n",
    "            plt.show()\n",
    "        validation_stats.append(final_stats)\n",
    "    return validation_stats, pd.concat(mco_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function, removing misclassified observations\n",
    "\n",
    "I personally think that this is cheating, but it has apparently been done by others, who claim to have great performance (95.416% accuracy, I have had 95.589% accuracy using this method) of their models. This would of course make sense, as you are removing observations, which are difficult to classify. See more:\n",
    "\n",
    "B.M. Patil, R.C. Joshi, Durga Toshniwal,\n",
    "Hybrid prediction model for Type-2 diabetic patients,\n",
    "https://www.sciencedirect.com/science/article/pii/S0957417410004896\n",
    "\n",
    "Wu, Han & Yang, Shengqi & Huang, Zhangqin & He, Jian & Wang, Xiaoyi. (2017). Type 2 diabetes mellitus prediction model based on data mining. Informatics in Medicine Unlocked. 10. 10.1016/j.imu.2017.12.006.\\\n",
    "https://www.researchgate.net/figure/Comparison-with-others-experiments_tbl2_321776861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_misclassified_observations(DataFrame):\n",
    "    global features\n",
    "\n",
    "    ## Prepare data for clustering\n",
    "    k_df = remove_outliers(remove_nan(DataFrame))\n",
    "\n",
    "    ## Instantialize the KMeans class and fit it on the data\n",
    "    kmeans = KMeans(n_clusters=2, n_init=50, max_iter=10_000)\n",
    "    kmeans.fit(k_df[features])\n",
    "\n",
    "    ## Give the dataframe a column with the labels from the clusterer\n",
    "    k_df = k_df.assign(cluster=kmeans.labels_)\n",
    "\n",
    "    ## Map the outcome to a cluster\n",
    "    mapping = k_df.groupby('cluster')['Outcome'].agg(lambda x: x.value_counts().index[0]).to_dict()\n",
    "    k_df['cluster_mapped_to_class'] = k_df['cluster'].map(mapping)\n",
    "\n",
    "    ## Create a boolean column, indicating whether or not the observation was misclassified\n",
    "    k_df['misclassified'] = k_df['Outcome'] != k_df['cluster_mapped_to_class']\n",
    "    misclassified_df = k_df[k_df['misclassified']]\n",
    "\n",
    "    print('Accuracy of clusterer', round(100-len(misclassified_df)/len(k_df)*100,2),'%')\n",
    "\n",
    "    ## Filter the dataframe for misclassified observations\n",
    "    misclassified_indices = misclassified_df.index\n",
    "    return DataFrame.copy().drop(misclassified_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_df(DataFrame):\n",
    "    global features\n",
    "\n",
    "    df_copy = DataFrame.copy()\n",
    "    cluster_list = np.full((len(df_copy),1), fill_value=-1)\n",
    "\n",
    "    ## Prepare data for clustering\n",
    "    k_df = remove_outliers(remove_nan(DataFrame))\n",
    "\n",
    "    ## Instantialize the KMeans class and fit it on the data\n",
    "    kmeans = KMeans(n_clusters=2, n_init=50, max_iter=10_000)\n",
    "    kmeans.fit(k_df[features])\n",
    "\n",
    "    ## Give the dataframe a column with the labels from the clusterer\n",
    "    k_df = k_df.assign(cluster=kmeans.labels_)\n",
    "\n",
    "    ## Map the outcome to a cluster\n",
    "    mapping = k_df.groupby('cluster')['Outcome'].agg(lambda x: x.value_counts().index[0]).to_dict()\n",
    "    k_df['cluster_mapped_to_class'] = k_df['cluster'].map(mapping)\n",
    "    indices = k_df.index\n",
    "    for idx in indices:\n",
    "        cluster_list[idx] = k_df.loc[idx][len(k_df.columns)-1]\n",
    "    df_copy = df_copy.assign(Cluster=cluster_list)\n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pvest\\anaconda3\\envs\\mldm\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "c:\\Users\\pvest\\anaconda3\\envs\\mldm\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pvest\\anaconda3\\envs\\mldm\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pvest\\anaconda3\\envs\\mldm\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pvest\\anaconda3\\envs\\mldm\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pvest\\anaconda3\\envs\\mldm\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 768 \n",
      "Length after processing: 720 \n",
      "Observations removed: 48\n",
      "Working on: CUDA\n",
      "1 0.79\n",
      "False Negative % 0.53 False Positive % 0.07 \n",
      "\n",
      "Misclassified observations: 30 / 142\n",
      "2 0.75\n",
      "False Negative % 0.48 False Positive % 0.11 \n",
      "\n",
      "Misclassified observations: 36 / 142\n",
      "3 0.75\n",
      "False Negative % 0.64 False Positive % 0.02 \n",
      "\n",
      "Misclassified observations: 36 / 145\n",
      "4 0.75\n",
      "False Negative % 0.68 False Positive % 0.02 \n",
      "\n",
      "Misclassified observations: 36 / 145\n",
      "5 0.79\n",
      "False Negative % 0.57 False Positive % 0.03 \n",
      "\n",
      "Misclassified observations: 30 / 146\n",
      "Mean Accuracy: 76.664 % Std 0.02 Min/Max 0.75 0.79 Mean CI: 0.7 0.84 Mean False Negatives: 0.58 Mean False Positives: 0.05\n"
     ]
    }
   ],
   "source": [
    "## Decide on k\n",
    "k = 5\n",
    "\n",
    "## Define threshold of standard deviations from mean to remove\n",
    "std_threshold = 4\n",
    "\n",
    "## Define how many nearest neighbors to use in potential imputation\n",
    "n_nearest_neighbors = 3\n",
    "\n",
    "## How many times to go train on the dataset before evaluating?\n",
    "rounds_to_train = 5\n",
    "\n",
    "## How many principal components should be used?\n",
    "number_of_pcs = 0\n",
    "\n",
    "## Learning Rate\n",
    "LR = 0.000009\n",
    "\n",
    "# Which columns to drop when standardizing and giving to model. 'Outcome' should be dropped\n",
    "# If any other columns shold be dropped (for feature engineering purposes), these can be specified here as well:\n",
    "drop_columns = ['Outcome', 'Insulin', 'SkinThickness']\n",
    "\n",
    "# Which columns to log-transform?\n",
    "log_columns = ['Pregnancies', 'Insulin', 'DiabetesPedigreeFunction', 'Age']\n",
    "\n",
    "# Get all remaining columns in lists\n",
    "features = [col for col in df.columns if col not in [kol for kol in drop_columns]]\n",
    "\n",
    "\n",
    "## Create Data from DataFrame\n",
    "X_data, Y_data, pc_array = k_split(df, k=k, RemoveOutliers=True, ImputeMissing=False, log_transform=log_columns,\n",
    "                                            remove_misclassified = False, add_clustering=True)\n",
    "\n",
    "## Perform cross validation\n",
    "statistics, misclassified_observations = cross_validate(X_data,Y_data,pc_array, n_pcs=number_of_pcs, \n",
    "                                                learning_rate = LR, rtt=rounds_to_train, plot=False)\n",
    "\n",
    "## Analyze the data\n",
    "means, low_means, high_means, false_negatives, false_positives = [], [], [], [], []\n",
    "for run_stat in statistics:\n",
    "    means.append(run_stat[0])\n",
    "    low_means.append(run_stat[1])\n",
    "    high_means.append(run_stat[2])\n",
    "    false_negatives.append(1-run_stat[3])\n",
    "    false_positives.append(1-run_stat[4])\n",
    "        \n",
    "print('Mean Accuracy:', np.round(np.mean(means)*100,3),'%', 'Std', np.round(np.std(means),2), 'Min/Max', np.round(min(means),2), \n",
    "    np.round(max(means),2), 'Mean CI:', np.round(np.mean(low_means),2), np.round(np.mean(high_means),2),\n",
    "    'Mean False Negatives:', np.round(np.mean(false_negatives),2), \n",
    "    'Mean False Positives:', np.round(np.mean(false_positives),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\n",
      "\n",
      "Neural Network Accuracy: 77.361 %\n",
      "Naive Accuracy w/ nn predictions: 80.282 % \n",
      "Naive Accuracy without nn preds: 78.873 %\n",
      "\n",
      "1:\n",
      "\n",
      "Neural Network Accuracy: 77.469 %\n",
      "Naive Accuracy w/ nn predictions: 78.169 % \n",
      "Naive Accuracy without nn preds: 74.648 %\n",
      "\n",
      "2:\n",
      "\n",
      "Neural Network Accuracy: 75.522 %\n",
      "Naive Accuracy w/ nn predictions: 81.379 % \n",
      "Naive Accuracy without nn preds: 80.69 %\n",
      "\n",
      "3:\n",
      "\n",
      "Neural Network Accuracy: 76.389 %\n",
      "Naive Accuracy w/ nn predictions: 74.483 % \n",
      "Naive Accuracy without nn preds: 74.483 %\n",
      "\n",
      "4:\n",
      "\n",
      "Neural Network Accuracy: 77.747 %\n",
      "Naive Accuracy w/ nn predictions: 71.918 % \n",
      "Naive Accuracy without nn preds: 70.548 %\n",
      "\n",
      "Mean Naive Accuracy with nn predictions: 77.246 % \n",
      "Mean nn Accuracy on test data: 76.664 % \n",
      "Mean nn Accuracy on test+train data: 76.897 % \n",
      "Mean Naive Accuracy without nn predictions: 75.848 %\n"
     ]
    }
   ],
   "source": [
    "with_nn_acc = []\n",
    "no_nn_acc = []\n",
    "nn_acc_list = []\n",
    "\n",
    "for data_idx in range(k):\n",
    "    print(f'{data_idx}:\\n')\n",
    "    ALL_data = np.concatenate((X_data[data_idx], Y_data[data_idx]))\n",
    "\n",
    "    ## Agent gives predictions on all data, which can be used by the Naive Bayes classifier\n",
    "    agent = Agent(training=False, file_path=f'Classifier_models/model/{data_idx}_model.pth', data=ALL_data, \n",
    "                inputs=number_of_pcs)\n",
    "    nn_preds, nn_accuracy = agent.train(1, pcs=[pcs[data_idx][j] for j in range(number_of_pcs)], \n",
    "                            evaluating=True, give_preds=True)\n",
    "    nn_acc_list.append(nn_accuracy)\n",
    "    print('Neural Network Accuracy:', np.round(nn_accuracy*100,3), '%')\n",
    "\n",
    "    new_features = features\n",
    "    if split_clustering:\n",
    "        new_features = np.concatenate([features, ['Cluster']]).flatten()\n",
    "\n",
    "    X_train_no_nn = pd.DataFrame(X_data[data_idx].T[:len(X_data[data_idx].T)-1].T, \n",
    "                            columns=new_features)\n",
    "\n",
    "    X_train = X_train_no_nn.assign(nn_predictions=nn_preds[:len(X_data[data_idx])])\n",
    "\n",
    "\n",
    "    y_train = pd.DataFrame(X_data[data_idx].T[-1].T, columns=['Outcome'])\n",
    "\n",
    "    X_test_no_nn = pd.DataFrame(Y_data[data_idx].T[:len(X_data[data_idx].T)-1].T, \n",
    "                            columns=new_features)\n",
    "\n",
    "    X_test = X_test_no_nn.assign(nn_predictions=nn_preds[len(X_data[data_idx]):])\n",
    "\n",
    "    y_test = pd.DataFrame(Y_data[data_idx].T[-1].T, columns=['Outcome'])\n",
    "\n",
    "    naive_model = GaussianNB()\n",
    "    naive_model_no_nn = GaussianNB()\n",
    "\n",
    "    naive_model.fit(X_train, y_train.values.ravel())\n",
    "    naive_model_no_nn.fit(X_train_no_nn, y_train.values.ravel())\n",
    "\n",
    "\n",
    "    y_pred = naive_model.predict(X_test)\n",
    "    y_pred_no_nn = naive_model_no_nn.predict(X_test_no_nn)\n",
    "\n",
    "    accuray = accuracy_score(y_pred, y_test)\n",
    "    with_nn_acc.append(accuray)\n",
    "\n",
    "    accuray_no_nn = accuracy_score(y_pred_no_nn, y_test)\n",
    "    no_nn_acc.append(accuray_no_nn)\n",
    "    print('Naive Accuracy w/ nn predictions:', round(accuray*100,3), '%',\n",
    "            '\\nNaive Accuracy without nn preds:', round(accuray_no_nn*100,3), '%\\n')\n",
    "\n",
    "print('Mean Naive Accuracy with nn predictions:', round(np.mean(with_nn_acc)*100,3), '%',\n",
    "        '\\nMean nn Accuracy on test data:', np.round(np.mean(means)*100,3),'%',\n",
    "        '\\nMean nn Accuracy on test+train data:', np.round(np.mean(nn_acc_list)*100,3),'%',\n",
    "        '\\nMean Naive Accuracy without nn predictions:', round(np.mean(no_nn_acc)*100,3), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
