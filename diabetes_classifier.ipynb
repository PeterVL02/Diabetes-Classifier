{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import os\n",
    "import seaborn as sns\n",
    "import random\n",
    "import torch.cuda\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_QNet(nn.Module):\n",
    "    def __init__(self, inputs):\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.linear1 = nn.Linear(8 + inputs, 256)\n",
    "        self.linear2 = nn.Linear(256,512)\n",
    "        self.linear3 = nn.Linear(512, 256)\n",
    "        self.linear4 = nn.Linear(256,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.linear1(x))\n",
    "        x = F.tanh(self.linear2(x))\n",
    "        x = F.tanh(self.linear3(x))\n",
    "        x = F.tanh(self.linear4(x))\n",
    "        return x\n",
    "    \n",
    "    def save(self, file_name='model.pth', index=0):\n",
    "        model_folder_path = './Classifier_models/model' \n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        complete_file_name = f\"{index}_{file_name}\"\n",
    "        file_path = os.path.join(model_folder_path, complete_file_name)\n",
    "        \n",
    "        torch.save(self.state_dict(), file_path)\n",
    "    \n",
    "class QTrainer:\n",
    "    def __init__(self, model, lr):\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        self.device = model.device\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def train_step(self, state, action, reward, done, outcome):\n",
    "        state = torch.tensor(np.array(state), dtype=torch.float).to(self.device)\n",
    "        action = torch.tensor(np.array(action), dtype=torch.long).to(self.device)\n",
    "        reward = torch.tensor(np.array(reward), dtype=torch.float).to(self.device)\n",
    "        # (n, x)\n",
    "\n",
    "        if len(state.shape) == 1:\n",
    "            # (1, x)\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            action = torch.unsqueeze(action, 0)\n",
    "            reward = torch.unsqueeze(reward, 0)\n",
    "            done = (done, )\n",
    "\n",
    "        # 1: predicted Q values with current state\n",
    "        pred = self.model(state)\n",
    "        target = pred.clone()\n",
    "        \n",
    "        if outcome[0].item() == 1:\n",
    "            target[0,0] = 1\n",
    "            target[0,1] = 0\n",
    "        else:\n",
    "            target[0,0] = 0\n",
    "            target[0,1] = 1\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        ## Calculate MSE loss on target and prediction\n",
    "        loss = self.criterion(target, pred)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "## Agent tager filepath som input hvis man vil køre en model, der allerede er trænet.\n",
    "class Agent:\n",
    "    def __init__(self, inputs, file_path=None, training=True, device=None, \n",
    "                 learning_rate=0.01, model_name='testing', data=None):\n",
    "        self.data = data\n",
    "        self.MAX_MEMORY = 5_000 ## Længde af buffer\n",
    "        self.memory = deque(maxlen=self.MAX_MEMORY)  ## popleft() buffer\n",
    "        self.BATCH_SIZE = 32 ## Sample størrelse\n",
    "        self.LR = learning_rate ## Learning rate (TIDLIGERE 0.01 for onestep)\n",
    "        if device is not None: ## Her kan man vælge at køre cpu selvom man har cuda\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = Linear_QNet(inputs=inputs).to(self.device) \n",
    "        # print(\"Working on:\", f\"{self.device}\".upper())\n",
    "        self.model_name = model_name\n",
    "\n",
    "        ## Definerer en masse variable baseret på __init__ input\n",
    "        self.is_training = training\n",
    "        self.file_path = file_path\n",
    "\n",
    "        ## Hvis vi har en sti til en model, vil vi loade den i stedet for at træne en ny\n",
    "        if self.file_path is not None:\n",
    "            self.model.load_state_dict(torch.load(self.file_path, map_location=self.device))\n",
    "            self.model.eval()\n",
    "\n",
    "        ## Initialisér trainer\n",
    "        self.trainer = QTrainer(self.model, lr=self.LR)\n",
    "\n",
    "\n",
    "        ## Gemmer state-actionpar til buffer\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        ## Træner long memory\n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > self.BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, self.BATCH_SIZE) # list of tuples\n",
    "        else:\n",
    "            mini_sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, dones, self.target_model)\n",
    "\n",
    "        ## Træner short memory\n",
    "    def train_short_memory(self, state, action, reward, done, outcome):\n",
    "        self.trainer.train_step(state, action, reward, done, outcome)\n",
    "\n",
    "        ## Bestem en action\n",
    "    def get_action(self, state):\n",
    "\n",
    "        final_move = torch.tensor([0,0])\n",
    "\n",
    "        ## Laver state om til tensor og får en prediction fra modellen\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float).to(self.device).clone().detach()\n",
    "        prediction = self.model(state_tensor)\n",
    "\n",
    "        ## Vælger den class med højeste værdi\n",
    "        move = torch.argmax(prediction).item()\n",
    "        final_move[move] = 1\n",
    "\n",
    "        return final_move\n",
    "    \n",
    "    def train(self, rounds, pcs, loading=False):\n",
    "        accur = np.zeros(rounds)\n",
    "        lower_acc = np.zeros(rounds)\n",
    "        upper_acc = np.zeros(rounds)\n",
    "        pos_outcome_tracker = []\n",
    "        neg_outcome_tracker = []\n",
    "        for j in range(rounds):\n",
    "            res = []\n",
    "            for i in range(len(self.data)):\n",
    "                state = self.data[i,:8]\n",
    "                pc_vals = [pc @ state for pc in pcs]\n",
    "                state = np.concatenate((state, pc_vals))\n",
    "                outcome = self.data[i,-1]\n",
    "                outcome = torch.tensor([1,0] if outcome == 1 else [0,1])\n",
    "                decision = self.get_action(state)\n",
    "                reward = 0\n",
    "                if list(decision) == list(outcome): reward = 1\n",
    "\n",
    "                if list(outcome) == [1,0]:\n",
    "                    ## Personen fik diabetes\n",
    "                    if list(decision) == [1,0]: pos_outcome_tracker.append(1)\n",
    "                    else: pos_outcome_tracker.append(0)\n",
    "                else:\n",
    "                    ## Personen fik ikke diabetes\n",
    "                    if list(decision) == [0,1]: neg_outcome_tracker.append(1)\n",
    "                    else: neg_outcome_tracker.append(0)\n",
    "\n",
    "                if self.is_training: self.train_short_memory(state, decision, reward, done=True, outcome=outcome)\n",
    "                res.append(reward)\n",
    "\n",
    "            p_hat = np.mean(res)\n",
    "            lower, upper =p_hat - 1.96*np.sqrt((p_hat*(1-p_hat))/len(res)), p_hat + 1.96*np.sqrt((p_hat*(1-p_hat))/len(res))\n",
    "\n",
    "            if j % (rounds/20) == 0 and loading:\n",
    "                print(f'Loading: {int((j)/rounds*100)}%'*self.is_training,'Mean', round(np.mean(res),2), \n",
    "                      'CI:', round(lower,2), round(upper,2))\n",
    "                \n",
    "            accur[j] = np.mean(res)\n",
    "            lower_acc[j], upper_acc[j] = lower, upper\n",
    "        self.model.save()\n",
    "        pos_accuracy = np.mean(pos_outcome_tracker)\n",
    "        neg_accuracy = np.mean(neg_outcome_tracker)\n",
    "\n",
    "        return np.concatenate(([accur], [lower_acc], [upper_acc]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file to pd DataFrame\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Replace 0's with NaN in columns where 0's are not possible\n",
    "for col in df.columns:\n",
    "    if col in ['Pregnancies', 'Outcome']:\n",
    "        continue\n",
    "    df[col] = df[col].replace(0, np.nan)\n",
    "\n",
    "# Remove NaN rows in data\n",
    "def remove_nan(DataFrame):\n",
    "    return DataFrame.dropna()\n",
    "\n",
    "def impute_nan(DataFrame, n=5):\n",
    "    imputer = KNNImputer(n_neighbors=n)\n",
    "    imputed_array = imputer.fit_transform(DataFrame)\n",
    "    imputed_df = pd.DataFrame(data=imputed_array, columns=DataFrame.columns)\n",
    "    return  imputed_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(DataFrame, std_threshold=4):\n",
    "    # Calculate the mean and standard deviation for each column\n",
    "    means = DataFrame.mean()\n",
    "    stds = DataFrame.std()\n",
    "\n",
    "    # Define threshold as 4 standard deviations from mean\n",
    "    std_threshold = 4\n",
    "\n",
    "    # Create boolean DataFrame indicating whether or not observations exceed threshold\n",
    "    conditions = (DataFrame < (means - std_threshold * stds)) | (DataFrame > (means + std_threshold * stds))\n",
    "\n",
    "    # Any row that should be removed will have at least one True in the conditions DataFrame\n",
    "    rows_to_remove = conditions.any(axis=1)\n",
    "\n",
    "    # Remove the rows that meet the condition\n",
    "    DataFrame = DataFrame[~rows_to_remove]\n",
    "    return DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into k groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which coloumns to drop when standardizing:\n",
    "drop_coloumns = ['Outcome']\n",
    "\n",
    "# Get all remaining columns in lists\n",
    "features = [col for col in df.columns if col not in [kol for kol in drop_coloumns]]\n",
    "\n",
    "def k_split(DataFrame, k, RemoveOutliers=False, ImputeMissing=False):\n",
    "    'If not ImputeMissing, NaNs will be removed. NaNs will always be removed in test data.'\n",
    "    # Shuffle the DataFrame\n",
    "    shuffled = DataFrame.sample(frac=1)\n",
    "\n",
    "    # Split into k groups\n",
    "    groups = np.array_split(shuffled, k)\n",
    "\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    pc_array = []\n",
    "    global features\n",
    "\n",
    "    for i in range(k):\n",
    "        groups_copy = groups.copy()\n",
    "        test_data = groups_copy.pop(i)\n",
    "\n",
    "        ## We do not want to impute the NaNs on the data, we are using to evaluate the model.\n",
    "        ## In this case, NaNs will be removed. Outliers will stay - may want to change that.\n",
    "        test_data = remove_nan(DataFrame=test_data)\n",
    "        if RemoveOutliers:\n",
    "            test_data = remove_outliers(DataFrame=test_data)\n",
    "        test_standard = test_data.loc[:, features].values\n",
    "        test_standard = StandardScaler().fit_transform(test_standard)\n",
    "        test_standard = np.column_stack((test_standard, test_data['Outcome']))\n",
    "        Y_data.append(test_standard)\n",
    "\n",
    "        training_frames = pd.concat(groups_copy)\n",
    "        if ImputeMissing:\n",
    "            training_frames = impute_nan(DataFrame=training_frames)\n",
    "        else:\n",
    "            training_frames = remove_nan(DataFrame=training_frames)\n",
    "        if RemoveOutliers:\n",
    "            training_frames = remove_outliers(DataFrame=training_frames)\n",
    "        training_standard = training_frames.loc[:, features].values\n",
    "        training_standard = StandardScaler().fit_transform(training_standard)\n",
    "        training_standard = np.column_stack((training_standard, training_frames['Outcome']))\n",
    "        X_data.append(training_standard)\n",
    "\n",
    "        ## Perform PCA\n",
    "        %run pca.ipynb \n",
    "        data_drop_outcome = np.array([row[:-1] for row in X_data[i]])\n",
    "        Vh = get_PCs(data_drop_outcome)\n",
    "        pc_array.append(Vh)\n",
    "\n",
    "    return X_data, Y_data, np.array(pc_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the cross-validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(training_data, test_data, pcs, n_pcs, learning_rate, rtt, plot=False, loading=False):\n",
    "    validation_stats = []\n",
    "    for i in range(len(training_data)):\n",
    "        X= training_data[i]\n",
    "        Y = test_data[i]\n",
    "\n",
    "        components = [pcs[i][j] for j in range(n_pcs)]\n",
    "\n",
    "\n",
    "        agent = Agent(learning_rate=learning_rate, data=X, inputs=n_pcs)\n",
    "        training_accuracy = agent.train(rtt, loading=loading, pcs=components)\n",
    "        if plot:\n",
    "            sns.lineplot(pd.DataFrame(training_accuracy.T, columns=['Accuracy', 'CI Lower Bound', 'CI Upper Bound']))\n",
    "            plt.xlabel('Rounds')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Training Accuracy')\n",
    "\n",
    "        if loading: print('\\nEval')\n",
    "        agent = Agent(training=False, file_path='Classifier_models/model/0_model.pth', data=Y, inputs=n_pcs)\n",
    "        final_stats = agent.train(1, loading=loading, pcs=components)\n",
    "        final_accuracy, final_lower_bound, final_upper_bound = [stat[0] for stat in final_stats]\n",
    "        print(i+1,round(final_accuracy,2))\n",
    "\n",
    "        if plot:\n",
    "            plt.errorbar(x=rtt, y= final_accuracy, yerr=(final_upper_bound-final_lower_bound)/2, elinewidth=1, \n",
    "            capsize=10)\n",
    "            plt.show()\n",
    "        validation_stats.append(final_stats)\n",
    "    return validation_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pvest\\anaconda3\\envs\\mldm\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.75\n",
      "2 0.77\n",
      "3 0.74\n",
      "4 0.84\n",
      "5 0.74\n",
      "Mean: 0.77 Std 0.04 Min/Max [0.74] [0.84] Mean CI: 0.67 0.86\n"
     ]
    }
   ],
   "source": [
    "df = remove_nan(df)\n",
    "df = remove_outliers(df)\n",
    "X_data, Y_data, pc_array = k_split(df, 5, RemoveOutliers=True, ImputeMissing=True)\n",
    "X_data, Y_data, pc_array\n",
    "\n",
    "## How many times to go train on the dataset before evaluating?\n",
    "rounds_to_train = 10\n",
    "\n",
    "## How many principal components should be used?\n",
    "number_of_pcs = 1\n",
    "\n",
    "## Learning Rate\n",
    "LR = 0.0001\n",
    "\n",
    "## Perform cross validation\n",
    "statistics = cross_validate(X_data,Y_data,pc_array, n_pcs=number_of_pcs, learning_rate = LR, rtt=rounds_to_train)\n",
    "\n",
    "## Analyze the data\n",
    "means, low_means, high_means = [], [], []\n",
    "for run_stat in statistics:\n",
    "    means.append(run_stat[0])\n",
    "    low_means.append(run_stat[1])\n",
    "    high_means.append(run_stat[2])\n",
    "print('Mean:', np.round(np.mean(means),2), 'Std', np.round(np.std(means),2), 'Min/Max', np.round(min(means),2), \n",
    "      np.round(max(means),2), 'Mean CI:', np.round(np.mean(low_means),2), np.round(np.mean(high_means),2))\n",
    "\n",
    "\n",
    "# lr 0.0001, 50 runder: Mean 0.83 CI: 0.75 0.92 \n",
    "# lr 0.0001, 20 runder, pc1 + pc2: Mean 0.85 CI: 0.77 0.93  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
